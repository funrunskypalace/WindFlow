/******************************************************************************
 *  This program is free software; you can redistribute it and/or modify it
 *  under the terms of the GNU Lesser General Public License version 3 as
 *  published by the Free Software Foundation.
 *  
 *  This program is distributed in the hope that it will be useful, but WITHOUT
 *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
 *  License for more details.
 *  
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program; if not, write to the Free Software Foundation,
 *  Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 ******************************************************************************
 */

/** 
 *  @file    builders_gpu.hpp
 *  @author  Gabriele Mencagli
 *  @date    02/04/2020
 *  
 *  @brief Builders used to create WindFlow operators working on GPU
 *  
 *  @section Builders-2 (Description)
 *  
 *  Set of builders used to create WindFlow operators on GPU.
 */ 

#ifndef BUILDERS_GPU_H
#define BUILDERS_GPU_H

/// includes
#include<chrono>
#include<memory>
#include<functional>
#include<meta.hpp>
#include<basic.hpp>
#include<meta_gpu.hpp>

namespace wf {

/** 
 *  \class WinSeqGPU_Builder
 *  
 *  \brief Builder of the Win_Seq_GPU node
 *  
 *  Builder class to ease the creation of the Win_Seq_GPU node.
 */ 
template<typename F_t>
class WinSeqGPU_Builder
{
private:
    F_t func;
    // extract the type of the node to be generated by this builder (with static checks)
    using tuple_t = decltype(get_tuple_t_WinGPU(func));
    using result_t = decltype(get_result_t_WinGPU(func));
    // static assert to check the signature
    static_assert(!(std::is_same<tuple_t, std::false_type>::value || std::is_same<result_t, std::false_type>::value),
        "WindFlow Compilation Error - unknown signature passed to the WinSeqGPU_Builder:\n"
        "  Candidate : __host__ __device__ void(uint64_t, const tuple_t *, size_t, result_t *, char *, size_t)\n");
    using winseq_gpu_t = Win_Seq_GPU<tuple_t, result_t, F_t>;
    uint64_t win_len = 1;
    uint64_t slide_len = 1;
    uint64_t triggering_delay = 0;
    win_type_t winType = win_type_t::CB;
    size_t batch_len = 1;
    int gpu_id = 0;
    size_t n_thread_block = DEFAULT_CUDA_NUM_THREAD_BLOCK;
    std::string name = "seq_gpu";
    size_t scratchpad_size = 0;

public:
    /** 
     *  \brief Constructor
     *  
     *  \param _func the non-incremental window processing logic (__host__ __device__)
     */ 
    WinSeqGPU_Builder(F_t _func): func(_func) {}

    /** 
     *  \brief Method to specify the configuration for count-based windows
     *  
     *  \param _win_len window length (in no. of tuples)
     *  \param _slide_len slide length (in no. of tuples)
     *  \return the object itself
     */ 
    WinSeqGPU_Builder<F_t> &withCBWindows(uint64_t _win_len, uint64_t _slide_len)
    {
        win_len = _win_len;
        slide_len = _slide_len;
        winType = win_type_t::CB;
        return *this;
    }

    /** 
     *  \brief Method to specify the configuration for time-based windows
     *  
     *  \param _win_len window length (in microseconds)
     *  \param _slide_len slide length (in microseconds)
     *  \param _triggering_delay (in microseconds)
     *  \return the object itself
     */ 
    WinSeqGPU_Builder<F_t> &withTBWindows(std::chrono::microseconds _win_len,
                                          std::chrono::microseconds _slide_len,
                                          std::chrono::microseconds _triggering_delay=std::chrono::microseconds::zero())
    {
        win_len = _win_len.count();
        slide_len = _slide_len.count();
        triggering_delay = _triggering_delay.count();
        winType = win_type_t::TB;
        return *this;
    }

    /** 
     *  \brief Method to specify the batch size
     *  
     *  \param _batch_len number of windows in a batch
     *  \return the object itself
     */ 
    WinSeqGPU_Builder<F_t> &withBatch(size_t _batch_len)
    {
        batch_len = _batch_len;
        return *this;
    }

    /** 
     *  \brief Method to specify the GPU configuration used to launch kernels
     *  
     *  \param _gpu_id identifier of the chosen GPU device (default zero)
     *  \param _n_thread_block number of threads per block (default is DEFAULT_CUDA_NUM_THREAD_BLOCK)
     *  \return the object itself
     */ 
    WinSeqGPU_Builder<F_t> &withGPUConfiguration(int _gpu_id=0, size_t _n_thread_block=DEFAULT_CUDA_NUM_THREAD_BLOCK)
    {
        gpu_id = _gpu_id;
        n_thread_block = _n_thread_block;
        return *this;
    }

    /** 
     *  \brief Method to specify the name of the Win_Seq_GPU node
     *  
     *  \param _name string with the name to be given
     *  \return the object itself
     */ 
    WinSeqGPU_Builder<F_t> &withName(std::string _name)
    {
        name = _name;
        return *this;
    }

    /** 
     *  \brief Method to specify the size in bytes of the scratchpad memory per CUDA thread
     *  
     *  \param _scratchpad_size size in bytes of the scratchpad area local of a CUDA thread (pre-allocated on the global memory of the GPU)
     *  \return the object itself
     */ 
    WinSeqGPU_Builder<F_t> &withScratchpad(size_t _scratchpad_size)
    {
        scratchpad_size = _scratchpad_size;
        return *this;
    }

#if __cplusplus >= 201703L
    /** 
     *  \brief Method to create the Win_Seq_GPU node (only C++17)
     *  
     *  \return a copy of the created Win_Seq_GPU node
     */ 
    winseq_gpu_t build()
    {
        return winseq_gpu_t(func,
                            win_len,
                            slide_len,
                            triggering_delay,
                            winType,
                            batch_len,
                            gpu_id,
                            n_thread_block,
                            name,
                            scratchpad_size);
    }
#endif

    /** 
     *  \brief Method to create the Win_Seq_GPU node
     *  
     *  \return a pointer to the created Win_Seq_GPU node (to be explicitly deallocated/destroyed)
     */ 
    winseq_gpu_t *build_ptr()
    {
        return new winseq_gpu_t(func,
                                win_len,
                                slide_len,
                                triggering_delay,
                                winType,
                                batch_len,
                                gpu_id,
                                n_thread_block,
                                name,
                                scratchpad_size);
    }

    /** 
     *  \brief Method to create the Win_Seq_GPU node
     *  
     *  \return a unique_ptr to the created Win_Seq_GPU node
     */ 
    std::unique_ptr<winseq_gpu_t> build_unique()
    {
        return std::make_unique<winseq_gpu_t>(func,
                                              win_len,
                                              slide_len,
                                              triggering_delay,
                                              winType,
                                              batch_len,
                                              gpu_id,
                                              n_thread_block,
                                              name,
                                              scratchpad_size);
    }
};

/** 
 *  \class WinSeqFFATGPU_Builder
 *  
 *  \brief Builder of the WinSeqFFAT_GPU node
 *  
 *  Builder class to ease the creation of the WinSeqFFAT_GPU node.
 */ 
template<typename F_t, typename G_t>
class WinSeqFFATGPU_Builder
{
private:
    F_t lift_func;
    G_t comb_func;
    // extract the type of the node to be generated by this builder (with static checks)
    using tuple_t = decltype(get_tuple_t_Lift(lift_func));
    using result_t = decltype(get_result_t_Lift(lift_func));
    // static asserts to check the signatures
    static_assert(!(std::is_same<tuple_t, std::false_type>::value || std::is_same<result_t, std::false_type>::value),
        "WindFlow Compilation Error - unknown signature passed to the WinSeqFFAT_Builder (first argument, lift logic):\n"
        "  Candidate 1 : void(const tuple_t &, result_t &)\n"
        "  Candidate 2 : void(const tuple_t &, result_t &, RuntimeContext &)\n");
    using result_t2 = decltype(get_result_t_Comb(comb_func));
    static_assert(!(std::is_same<std::false_type, result_t2>::value),
        "WindFlow Compilation Error - unknown signature passed to the WinSeqFFAT_Builder (second argument, combine logic):\n"
        "  Candidate : __host__ __device__ void(const result_t &, const result_t &, result_t &)\n");
    static_assert(std::is_same<result_t, result_t2>::value,
        "WindFlow Compilation Error - type mismatch in the WinSeqFFAT_Builder (output type of the lift logic must be equal to the input type of the combine logic)\n");
    using winffat_gpu_t = Win_SeqFFAT_GPU<tuple_t, result_t, G_t>;
    uint64_t win_len = 1;
    uint64_t slide_len = 1;
    uint64_t triggering_delay = 0;
    win_type_t winType = win_type_t::CB;
    size_t batch_len = 1;
    int gpu_id = 0;
    size_t n_thread_block = DEFAULT_CUDA_NUM_THREAD_BLOCK;
    bool rebuild = false;
    std::string name = "seqffat_gpu";

public:
    /** 
     *  \brief Constructor
     *  
     *  \param _lift_func the lift logic to translate a tuple into a result
     *  \param _comb_func the combine logic to combine two results into a result (__host__ __device__)
     */ 
    WinSeqFFATGPU_Builder(F_t _lift_func, G_t _comb_func): lift_func(_lift_func), comb_func(_comb_func) {}

    /** 
     *  \brief Method to specify the configuration for count-based windows
     *  
     *  \param _win_len window length (in no. of tuples)
     *  \param _slide_len slide length (in no. of tuples)
     *  \return the object itself
     */ 
    WinSeqFFATGPU_Builder<F_t, G_t> &withCBWindows(uint64_t _win_len, uint64_t _slide_len)
    {
        win_len = _win_len;
        slide_len = _slide_len;
        winType = win_type_t::CB;
        return *this;
    }

    /** 
     *  \brief Method to specify the configuration for time-based windows
     *  
     *  \param _win_len window length (in microseconds)
     *  \param _slide_len slide length (in microseconds)
     *  \param _triggering_delay (in microseconds)
     *  \return the object itself
     */ 
    WinSeqFFATGPU_Builder<F_t, G_t> &withTBWindows(std::chrono::microseconds _win_len,
                                                   std::chrono::microseconds _slide_len, 
                                                   std::chrono::microseconds _triggering_delay=std::chrono::microseconds::zero())
    {
        win_len = _win_len.count();
        slide_len = _slide_len.count();
        triggering_delay = _triggering_delay.count();
        winType = win_type_t::TB;
        return *this;
    }

    /** 
     *  \brief Method to specify the batch size
     *  
     *  \param _batch_len number of windows in a batch
     *  \return the object itself
     */ 
    WinSeqFFATGPU_Builder<F_t, G_t> &withBatch(size_t _batch_len)
    {
        batch_len = _batch_len;
        return *this;
    }

    /** 
     *  \brief Method to specify the GPU configuration used to launch kernels
     *  
     *  \param _gpu_id identifier of the chosen GPU device (default zero)
     *  \param _n_thread_block number of threads per block (default is DEFAULT_CUDA_NUM_THREAD_BLOCK)
     *  \return the object itself
     */ 
    WinSeqFFATGPU_Builder<F_t, G_t> &withGPUConfiguration(int _gpu_id=0, size_t _n_thread_block=DEFAULT_CUDA_NUM_THREAD_BLOCK)
    {
        gpu_id = _gpu_id;
        n_thread_block = _n_thread_block;
        return *this;
    }

    /** 
     *  \brief Method to specify the name of the Win_SeqFFAT_GPU node
     *  
     *  \param _name string with the name to be given
     *  \return the object itself
     */ 
    WinSeqFFATGPU_Builder<F_t, G_t> &withName(std::string _name)
    {
        name = _name;
        return *this;
    }

    /** 
     *  \brief Method to specify if the FlatFAT_GPU must recomputed from scratch for each batch
     *  
     *  \param _rebuild if true the FlatFAT_GPU structure is rebuilt for each batch (it is updated otherwise)
     *  \return the object itself
     */ 
    WinSeqFFATGPU_Builder<F_t, G_t> &withRebuild(bool _rebuild)
    {
        rebuild = _rebuild;
        return *this;
    }

#if __cplusplus >= 201703L
    /** 
     *  \brief Method to create the Win_SeqFFAT_GPU node (only C++17)
     *  
     *  \return a copy of the created Win_SeqFFAT_GPU node
     */ 
    winffat_gpu_t build()
    {
        return winffat_gpu_t(lift_func, 
                             comb_func,
                             win_len,
                             slide_len,
                             triggering_delay,
                             winType,
                             batch_len,
                             gpu_id,
                             n_thread_block,
                             rebuild,
                             name);
    }
#endif

    /** 
     *  \brief Method to create the Win_SeqFFAT_GPU node
     *  
     *  \return a pointer to the created Win_SeqFFAT_GPU node (to be explicitly deallocated/destroyed)
     */ 
    winffat_gpu_t *build_ptr()
    {
        return new winffat_gpu_t(lift_func, 
                                 comb_func,
                                 win_len,
                                 slide_len,
                                 triggering_delay,
                                 winType,
                                 batch_len,
                                 gpu_id,
                                 n_thread_block,
                                 rebuild,
                                 name);
    }

    /** 
     *  \brief Method to create the Win_SeqFFAT_GPU node
     *  
     *  \return a unique_ptr to the created Win_SeqFFAT_GPU node
     */ 
    std::unique_ptr<winffat_gpu_t> build_unique()
    {
        return std::make_unique<winffat_gpu_t>(lift_func,
                                               comb_func,
                                               win_len,
                                               slide_len,
                                               triggering_delay,
                                               winType,
                                               batch_len,
                                               gpu_id,
                                               n_thread_block,
                                               rebuild,
                                               name);
    }
};

/** 
 *  \class WinFarmGPU_Builder
 *  
 *  \brief Builder of the Win_Farm_GPU operator
 *  
 *  Builder class to ease the creation of the Win_Farm_GPU operator.
 */ 
template<typename T>
class WinFarmGPU_Builder
{
private:
    T &input;
    // extract the type of the operator to be generated by this builder (with static checks)
    using winfarm_gpu_t = std::remove_reference_t<decltype(*get_WF_GPU_nested_type(input))>;
    // static assert to check the signature
    static_assert(!std::is_same<winfarm_gpu_t, std::false_type>::value,
        "WindFlow Compilation Error - unknown signature passed to the WinFarmGPU_Builder:\n"
        "  Candidate 1 : __host__ __device__ void(uint64_t, const tuple_t *, size_t, result_t *, char *, size_t)\n"
        "  Candidate 2 : a valid Pane_Farm_GPU operator\n"
        "  Candidate 3 : a valid Win_MapReduce_GPU operator\n");
    uint64_t win_len = 1;
    uint64_t slide_len = 1;
    uint64_t triggering_delay = 0;
    win_type_t winType = win_type_t::CB;
    size_t pardegree = 1;
    size_t batch_len = 1;
    int gpu_id = 0;
    size_t n_thread_block = DEFAULT_CUDA_NUM_THREAD_BLOCK;
    std::string name = "wf_gpu";
    size_t scratchpad_size = 0;
    opt_level_t opt_level = opt_level_t::LEVEL2;
    bool isComplex=false;

    // window parameters initialization (input is a Pane_Farm_GPU)
    template<typename ...Args>
    void initWindowConf(Pane_Farm_GPU<Args...> &_pf)
    {
        win_len = _pf.win_len;
        slide_len = _pf.slide_len;
        triggering_delay = _pf.triggering_delay;
        winType = _pf.winType;
        batch_len = _pf.batch_len;
        gpu_id = _pf.gpu_id;
        n_thread_block = _pf.n_thread_block;
        isComplex = true;
    }

    // window parameters initialization (input is a Win_MapReduce_GPU)
    template<typename ...Args>
    void initWindowConf(Win_MapReduce_GPU<Args...> &_wmr)
    {
        win_len = _wmr.win_len;
        slide_len = _wmr.slide_len;
        triggering_delay = _wmr.triggering_delay;
        winType = _wmr.winType;
        batch_len = _wmr.batch_len;
        gpu_id = _wmr.gpu_id;
        n_thread_block = _wmr.n_thread_block;
        isComplex = true;
    }

    // window parameters initialization (input is a logic)
    template<typename T2>
    void initWindowConf(T2 &f)
    {
        win_len = 1;
        slide_len = 1;
        triggering_delay = 0;
        winType = win_type_t::CB;
        batch_len = 1;
        gpu_id = 0;
        n_thread_block = DEFAULT_CUDA_NUM_THREAD_BLOCK;
    }

public:
    /** 
     *  \brief Constructor
     *  
     *  \param _input can be either a non-incremental window processing logic (__host__ __device__) or an
     *                already instantiated Pane_Farm_GPU or Win_MapReduce_GPU operator.
     */ 
    WinFarmGPU_Builder(T &_input): input(_input) {
        initWindowConf(input);
    }

    /** 
     *  \brief Method to specify the configuration for count-based windows
     *  
     *  \param _win_len window length (in no. of tuples)
     *  \param _slide_len slide length (in no. of tuples)
     *  \return the object itself
     */ 
    WinFarmGPU_Builder<T> &withCBWindows(uint64_t _win_len, uint64_t _slide_len)
    {
        // check if it is a complex nesting
        if (isComplex) {
            std::cerr << RED << "WindFlow Error: nesting does not allow configuring the Win_Farm_GPU parameters" << DEFAULT_COLOR << std::endl;
            exit(EXIT_FAILURE);
        }
        win_len = _win_len;
        slide_len = _slide_len;
        winType = win_type_t::CB;
        return *this;
    }

    /** 
     *  \brief Method to specify the configuration for time-based windows
     *  
     *  \param _win_len window length (in microseconds)
     *  \param _slide_len slide length (in microseconds)
     *  \param _triggering_delay (in microseconds)
     *  \return the object itself
     */ 
    WinFarmGPU_Builder<T> &withTBWindows(std::chrono::microseconds _win_len,
                                         std::chrono::microseconds _slide_len,
                                         std::chrono::microseconds _triggering_delay=std::chrono::microseconds::zero())
    {
        // check if it is a complex nesting
        if (isComplex) {
            std::cerr << RED << "WindFlow Error: nesting does not allow configuring the Win_Farm_GPU parameters" << DEFAULT_COLOR << std::endl;
            exit(EXIT_FAILURE);
        }
        win_len = _win_len.count();
        slide_len = _slide_len.count();
        triggering_delay = _triggering_delay.count();
        winType = win_type_t::TB;
        return *this;
    }

    /** 
     *  \brief Method to specify the parallelism of the Win_Farm_GPU operator
     *  
     *  \param _pardegree number of replicas
     *  \return the object itself
     */ 
    WinFarmGPU_Builder<T> &withParallelism(size_t _pardegree)
    {
        pardegree = _pardegree;
        return *this;
    }

    /** 
     *  \brief Method to specify the batch size
     *  
     *  \param _batch_len number of windows in a batch
     *  \return the object itself
     */ 
    WinFarmGPU_Builder<T> &withBatch(size_t _batch_len)
    {
        // check if it is a complex nesting
        if (isComplex) {
            std::cerr << RED << "WindFlow Error: nesting does not allow configuring the Win_Farm_GPU parameters" << DEFAULT_COLOR << std::endl;
            exit(EXIT_FAILURE);
        }
        batch_len = _batch_len;
        return *this;
    }

    /** 
     *  \brief Method to specify the GPU configuration used to launch kernels
     *  
     *  \param _gpu_id identifier of the chosen GPU device (default zero)
     *  \param _n_thread_block number of threads per block (default is DEFAULT_CUDA_NUM_THREAD_BLOCK)
     *  \return the object itself
     */ 
    WinFarmGPU_Builder<T> &withGPUConfiguration(int _gpu_id=0, size_t _n_thread_block=DEFAULT_CUDA_NUM_THREAD_BLOCK)
    {
        // check if it is a complex nesting
        if (isComplex) {
            std::cerr << RED << "WindFlow Error: nesting does not allow configuring the Win_Farm_GPU parameters" << DEFAULT_COLOR << std::endl;
            exit(EXIT_FAILURE);
        }
        gpu_id = _gpu_id;
        n_thread_block = _n_thread_block;
        return *this;
    }

    /** 
     *  \brief Method to specify the name of the Win_Farm_GPU operator
     *  
     *  \param _name string with the name to be given
     *  \return the object itself
     */ 
    WinFarmGPU_Builder<T> &withName(std::string _name)
    {
        name = _name;
        return *this;
    }

    /** 
     *  \brief Method to specify the size in bytes of the scratchpad memory per CUDA thread
     *  
     *  \param _scratchpad_size size in bytes of the scratchpad area local of a CUDA thread (pre-allocated on the global memory of the GPU)
     *  \return the object itself
     */ 
    WinFarmGPU_Builder<T> &withScratchpad(size_t _scratchpad_size)
    {
        // check if it is a complex nesting
        if (isComplex) {
            std::cerr << RED << "WindFlow Error: nesting does not allow configuring the Win_Farm_GPU parameters" << DEFAULT_COLOR << std::endl;
            exit(EXIT_FAILURE);
        }
        scratchpad_size = _scratchpad_size;
        return *this;
    }

    /** 
     *  \brief Method to specify the optimization level to build the Win_Farm_GPU operator
     *  
     *  \param _opt_level (optimization level)
     *  \return the object itself
     */ 
    WinFarmGPU_Builder<T> &withOptLevel(opt_level_t _opt_level)
    {
        opt_level = _opt_level;
        return *this;
    }

#if __cplusplus >= 201703L
    /** 
     *  \brief Method to create the Win_Farm_GPU operator (only C++17)
     *  
     *  \return a copy of the created Win_Farm_GPU operator
     */ 
    winfarm_gpu_t build()
    {
        return winfarm_gpu_t(input,
                             win_len,
                             slide_len,
                             triggering_delay,
                             winType,
                             pardegree,
                             batch_len,
                             gpu_id,
                             n_thread_block,
                             name,
                             scratchpad_size,
                             true,
                             opt_level);
    }
#endif

    /** 
     *  \brief Method to create the Win_Farm_GPU operator
     *  
     *  \return a pointer to the created Win_Farm_GPU operator (to be explicitly deallocated/destroyed)
     */ 
    winfarm_gpu_t *build_ptr()
    {
        return new winfarm_gpu_t(input,
                                 win_len,
                                 slide_len,
                                 triggering_delay,
                                 winType,
                                 pardegree,
                                 batch_len,
                                 gpu_id,
                                 n_thread_block,
                                 name,
                                 scratchpad_size,
                                 true,
                                 opt_level);
    }

    /** 
     *  \brief Method to create the Win_Farm_GPU operator
     *  
     *  \return a unique_ptr to the created Win_Farm_GPU operator
     */ 
    std::unique_ptr<winfarm_gpu_t> build_unique()
    {
        return std::make_unique<winfarm_gpu_t>(input,
                                               win_len,
                                               slide_len,
                                               triggering_delay,
                                               winType,
                                               pardegree,
                                               batch_len,
                                               gpu_id,
                                               n_thread_block,
                                               name,
                                               scratchpad_size,
                                               true,
                                               opt_level);
    }
};

/** 
 *  \class KeyFarmGPU_Builder
 *  
 *  \brief Builder of the Key_Farm_GPU operator
 *  
 *  Builder class to ease the creation of the Key_Farm_GPU operator.
 */ 
template<typename T>
class KeyFarmGPU_Builder
{
private:
    T &input;
    // extract the type of the operator to be generated by this builder (with static checks)
    using keyfarm_gpu_t = std::remove_reference_t<decltype(*get_KF_GPU_nested_type(input))>;
    // static assert to check the signature
    static_assert(!std::is_same<keyfarm_gpu_t, std::false_type>::value,
        "WindFlow Compilation Error - unknown signature passed to the KeyFarmGPU_Builder:\n"
        "  Candidate 1 : __host__ __device__ void(uint64_t, const tuple_t *, size_t, result_t *, char *, size_t)\n"
        "  Candidate 2 : a valid Pane_Farm_GPU operator\n"
        "  Candidate 3 : a valid Win_MapReduce_GPU operator\n");
    // type of the function to map the key hashcode onto an identifier starting from zero to pardegree-1
    using routing_func_t = std::function<size_t(size_t, size_t)>;
    uint64_t win_len = 1;
    uint64_t slide_len = 1;
    uint64_t triggering_delay = 0;
    win_type_t winType = win_type_t::CB;
    size_t pardegree = 1;
    size_t batch_len = 1;
    int gpu_id = 0;
    size_t n_thread_block = DEFAULT_CUDA_NUM_THREAD_BLOCK;
    std::string name = "wf_gpu";
    size_t scratchpad_size = 0;
    routing_func_t routing_func = [](size_t k, size_t n) { return k%n; };
    opt_level_t opt_level = opt_level_t::LEVEL2;
    bool isComplex=false;

    // window parameters initialization (input is a Pane_Farm_GPU)
    template<typename ...Args>
    void initWindowConf(Pane_Farm_GPU<Args...> &_pf)
    {
        win_len = _pf.win_len;
        slide_len = _pf.slide_len;
        triggering_delay = _pf.triggering_delay;
        winType = _pf.winType;
        batch_len = _pf.batch_len;
        gpu_id = _pf.gpu_id;
        n_thread_block = _pf.n_thread_block;
        isComplex = true;
    }

    // window parameters initialization (input is a Win_MapReduce_GPU)
    template<typename ...Args>
    void initWindowConf(Win_MapReduce_GPU<Args...> &_wmr)
    {
        win_len = _wmr.win_len;
        slide_len = _wmr.slide_len;
        triggering_delay = _wmr.triggering_delay;
        winType = _wmr.winType;
        batch_len = _wmr.batch_len;
        gpu_id = _wmr.gpu_id;
        n_thread_block = _wmr.n_thread_block;
        isComplex = true;
    }

    // window parameters initialization (input is a logic)
    template<typename T2>
    void initWindowConf(T2 &f)
    {
        win_len = 1;
        slide_len = 1;
        triggering_delay = 0;
        winType = win_type_t::CB;
        batch_len = 1;
        gpu_id = 0;
        n_thread_block = DEFAULT_CUDA_NUM_THREAD_BLOCK;
    }

public:
    /** 
     *  \brief Constructor
     *  
     *  \param _input can be either a non-incremental window processing logic (__host__ __device__) or an
     *                already instantiated Pane_Farm_GPU or Win_MapReduce_GPU operator.
     */ 
    KeyFarmGPU_Builder(T &_input): input(_input) {
        initWindowConf(input);
    }

    /** 
     *  \brief Method to specify the configuration for count-based windows
     *  
     *  \param _win_len window length (in no. of tuples)
     *  \param _slide_len slide length (in no. of tuples)
     *  \return the object itself
     */ 
    KeyFarmGPU_Builder<T> &withCBWindows(uint64_t _win_len, uint64_t _slide_len)
    {
        // check if it is a complex nesting
        if (isComplex) {
            std::cerr << RED << "WindFlow Error: nesting does not allow configuring the Key_Farm_GPU parameters" << DEFAULT_COLOR << std::endl;
            exit(EXIT_FAILURE);
        }
        win_len = _win_len;
        slide_len = _slide_len;
        winType = win_type_t::CB;
        return *this;
    }

    /** 
     *  \brief Method to specify the configuration for time-based windows
     *  
     *  \param _win_len window length (in microseconds)
     *  \param _slide_len slide length (in microseconds)
     *  \param _triggering_delay (in microseconds)
     *  \return the object itself
     */ 
    KeyFarmGPU_Builder<T> &withTBWindows(std::chrono::microseconds _win_len,
                                         std::chrono::microseconds _slide_len,
                                         std::chrono::microseconds _triggering_delay=std::chrono::microseconds::zero())
    {
        // check if it is a complex nesting
        if (isComplex) {
            std::cerr << RED << "WindFlow Error: nesting does not allow configuring the Key_Farm_GPU parameters" << DEFAULT_COLOR << std::endl;
            exit(EXIT_FAILURE);
        }
        win_len = _win_len.count();
        slide_len = _slide_len.count();
        triggering_delay = _triggering_delay.count();
        winType = win_type_t::TB;
        return *this;
    }

    /** 
     *  \brief Method to specify the parallelism of the Key_Farm_GPU operator
     *  
     *  \param _pardegree number of replicas
     *  \return the object itself
     */ 
    KeyFarmGPU_Builder<T> &withParallelism(size_t _pardegree)
    {
        pardegree = _pardegree;
        return *this;
    }

    /** 
     *  \brief Method to specify the batch size
     *  
     *  \param _batch_len number of windows in a batch
     *  \return the object itself
     */ 
    KeyFarmGPU_Builder<T> &withBatch(size_t _batch_len)
    {
        // check if it is a complex nesting
        if (isComplex) {
            std::cerr << RED << "WindFlow Error: nesting does not allow configuring the Key_Farm_GPU parameters" << DEFAULT_COLOR << std::endl;
            exit(EXIT_FAILURE);
        }
        batch_len = _batch_len;
        return *this;
    }

    /** 
     *  \brief Method to specify the GPU configuration used to launch kernels
     *  
     *  \param _gpu_id identifier of the chosen GPU device (default zero)
     *  \param _n_thread_block number of threads per block (default is DEFAULT_CUDA_NUM_THREAD_BLOCK)
     *  \return the object itself
     */ 
    KeyFarmGPU_Builder<T> &withGPUConfiguration(int _gpu_id=0, size_t _n_thread_block=DEFAULT_CUDA_NUM_THREAD_BLOCK)
    {
        // check if it is a complex nesting
        if (isComplex) {
            std::cerr << RED << "WindFlow Error: nesting does not allow configuring the Key_Farm_GPU parameters" << DEFAULT_COLOR << std::endl;
            exit(EXIT_FAILURE);
        }
        gpu_id = _gpu_id;
        n_thread_block = _n_thread_block;
        return *this;
    }

    /** 
     *  \brief Method to specify the name of the Key_Farm_GPU operator
     *  
     *  \param _name string with the name to be given
     *  \return the object itself
     */ 
    KeyFarmGPU_Builder<T> &withName(std::string _name)
    {
        name = _name;
        return *this;
    }

    /** 
     *  \brief Method to specify the size in bytes of the scratchpad memory per CUDA thread
     *  
     *  \param _scratchpad_size size in bytes of the scratchpad area local of a CUDA thread (pre-allocated on the global memory of the GPU)
     *  \return the object itself
     */ 
    KeyFarmGPU_Builder<T> &withScratchpad(size_t _scratchpad_size)
    {
        // check if it is a complex nesting
        if (isComplex) {
            std::cerr << RED << "WindFlow Error: nesting does not allow configuring the Key_Farm_GPU parameters" << DEFAULT_COLOR << std::endl;
            exit(EXIT_FAILURE);
        }
        scratchpad_size = _scratchpad_size;
        return *this;
    }

    /** 
     *  \brief Method to specify the optimization level to build the Key_Farm_GPU operator
     *  
     *  \param _opt_level (optimization level)
     *  \return the object itself
     */ 
    KeyFarmGPU_Builder<T> &withOptLevel(opt_level_t _opt_level)
    {
        opt_level = _opt_level;
        return *this;
    }

#if __cplusplus >= 201703L
    /** 
     *  \brief Method to create the Key_Farm_GPU operator (only C++17)
     *  
     *  \return a copy of the created Key_Farm_GPU operator
     */ 
    keyfarm_gpu_t build()
    {
        return keyfarm_gpu_t(input,
                             win_len,
                             slide_len,
                             triggering_delay,
                             winType,
                             pardegree,
                             batch_len,
                             gpu_id,
                             n_thread_block,
                             name,
                             scratchpad_size,
                             routing_func,
                             opt_level);
    }
#endif

    /** 
     *  \brief Method to create the Key_Farm_GPU operator
     *  
     *  \return a pointer to the created Key_Farm_GPU operator (to be explicitly deallocated/destroyed)
     */ 
    keyfarm_gpu_t *build_ptr()
    {
        return new keyfarm_gpu_t(input,
                                 win_len,
                                 slide_len,
                                 triggering_delay,
                                 winType,
                                 pardegree,
                                 batch_len,
                                 gpu_id,
                                 n_thread_block,
                                 name,
                                 scratchpad_size,
                                 routing_func,
                                 opt_level);
    }

    /** 
     *  \brief Method to create the Key_Farm_GPU operator
     *  
     *  \return a unique_ptr to the created Key_Farm_GPU operator
     */ 
    std::unique_ptr<keyfarm_gpu_t> build_unique()
    {
        return std::make_unique<keyfarm_gpu_t>(input,
                                               win_len,
                                               slide_len,
                                               triggering_delay,
                                               winType,
                                               pardegree,
                                               batch_len,
                                               gpu_id,
                                               n_thread_block,
                                               name,
                                               scratchpad_size,
                                               routing_func,
                                               opt_level);
    }
};

/** 
 *  \class KeyFFATGPU_Builder
 *  
 *  \brief Builder of the Key_FFAT_GPU operator
 *  
 *  Builder class to ease the creation of the Key_FFFAT_GPU operator.
 */ 
template<typename F_t, typename G_t>
class KeyFFATGPU_Builder
{
private:
    F_t lift_func;
    G_t comb_func;
    // extract the type of the operator to be generated by this builder (with static checks)
    using tuple_t = decltype(get_tuple_t_Lift(lift_func));
    using result_t = decltype(get_result_t_Lift(lift_func));
    // static asserts to check the signatures
    static_assert(!(std::is_same<tuple_t, std::false_type>::value || std::is_same<result_t, std::false_type>::value),
        "WindFlow Compilation Error - unknown signature passed to the KeyFFATGPU_Builder (first argument, lift logic):\n"
        "  Candidate 1 : void(const tuple_t &, result_t &)\n"
        "  Candidate 2 : void(const tuple_t &, result_t &, RuntimeContext &)\n");
    using result_t2 = decltype(get_result_t_Comb(comb_func));
    static_assert(!(std::is_same<std::false_type, result_t2>::value),
        "WindFlow Compilation Error - unknown signature passed to the KeyFFATGPU_Builder (second argument, combine logic):\n"
        "  Candidate 1 : __host__ __device__ void(const result_t &, const result_t &, result_t &)\n");
    static_assert(std::is_same<result_t, result_t2>::value,
        "WindFlow Compilation Error - type mismatch in the KeyFFATGPU_Builder (output type of the lift logic must be equal to the input type of the combine logic)\n");
    using keyffat_gpu_t = Key_FFAT_GPU<tuple_t, result_t, G_t>;
    // type of the function to map the key hashcode onto an identifier starting from zero to pardegree-1
    using routing_func_t = std::function<size_t(size_t, size_t)>;
    uint64_t win_len = 1;
    uint64_t slide_len = 1;
    uint64_t triggering_delay = 0;
    win_type_t winType = win_type_t::CB;
    size_t pardegree = 1;
    size_t batch_len = 1;
    int gpu_id = 0;
    size_t n_thread_block = DEFAULT_CUDA_NUM_THREAD_BLOCK;
    bool rebuild = false;
    std::string name = "kff_gpu";
    routing_func_t routing_func = [](size_t k, size_t n) { return k%n; };

public:
    /** 
     *  \brief Constructor
     *  
     *  \param _lift_func the lift logic to translate a tuple into a result
     *  \param _comb_func the combine logic to combine two results into a result (__host__ __device__)
     */ 
    KeyFFATGPU_Builder(F_t _lift_func, G_t _comb_func): lift_func(_lift_func), comb_func(_comb_func) {}

    /** 
     *  \brief Method to specify the configuration for count-based windows
     *  
     *  \param _win_len window length (in no. of tuples)
     *  \param _slide_len slide length (in no. of tuples)
     *  \return the object itself
     */ 
    KeyFFATGPU_Builder<F_t, G_t> &withCBWindows(uint64_t _win_len, uint64_t _slide_len)
    {
        win_len = _win_len;
        slide_len = _slide_len;
        winType = win_type_t::CB;
        return *this;
    }

    /** 
     *  \brief Method to specify the configuration for time-based windows
     *  
     *  \param _win_len window length (in microseconds)
     *  \param _slide_len slide length (in microseconds)
     *  \param _triggering_delay (in microseconds)
     *  \return the object itself
     */ 
    KeyFFATGPU_Builder<F_t, G_t> &withTBWindows(std::chrono::microseconds _win_len,
                                                std::chrono::microseconds _slide_len,
                                                std::chrono::microseconds _triggering_delay=std::chrono::microseconds::zero())
    {
        win_len = _win_len.count();
        slide_len = _slide_len.count();
        triggering_delay = _triggering_delay.count();
        winType = win_type_t::TB;
        return *this;
    }

    /** 
     *  \brief Method to specify the parallelism of the Key_FFAT_GPU operator
     *  
     *  \param _pardegree number of replicas
     *  \return the object itself
     */ 
    KeyFFATGPU_Builder<F_t, G_t> &withParallelism(size_t _pardegree)
    {
        pardegree = _pardegree;
        return *this;
    }

    /** 
     *  \brief Method to specify the batch size
     *  
     *  \param _batch_len number of windows in a batch
     *  \return the object itself
     */ 
    KeyFFATGPU_Builder<F_t, G_t> &withBatch(size_t _batch_len)
    {
        batch_len = _batch_len;
        return *this;
    }

    /** 
     *  \brief Method to specify the GPU configuration used to launch kernels
     *  
     *  \param _gpu_id identifier of the chosen GPU device (default zero)
     *  \param _n_thread_block number of threads per block (default is DEFAULT_CUDA_NUM_THREAD_BLOCK)
     *  \return the object itself
     */ 
    KeyFFATGPU_Builder<F_t, G_t> &withGPUConfiguration(int _gpu_id=0, size_t _n_thread_block=DEFAULT_CUDA_NUM_THREAD_BLOCK)
    {
        gpu_id = _gpu_id;
        n_thread_block = _n_thread_block;
        return *this;
    }

    /** 
     *  \brief Method to specify the name of the Key_FFAT_GPU operator
     *  
     *  \param _name string with the name to be given
     *  \return the object itself
     */ 
    KeyFFATGPU_Builder<F_t, G_t> &withName(std::string _name)
    {
        name = _name;
        return *this;
    }

    /** 
     *  \brief Method to specify if the FlatFAT_GPU must recomputed from scratch for each batch
     *  
     *  \param _rebuild if true the FlatFAT_GPU structure is rebuilt for each batch (it is updated otherwise)
     *  \return the object itself
     */ 
    KeyFFATGPU_Builder<F_t, G_t> &withRebuild(bool _rebuild)
    {
        rebuild = _rebuild;
        return *this;
    }

#if __cplusplus >= 201703L
    /** 
     *  \brief Method to create the Key_FFAT_GPU operator (only C++17)
     *  
     *  \return a copy of the created Key_FFAT_GPU operator
     */ 
    keyffat_gpu_t build()
    {
        return keyffat_gpu_t(lift_func,
                             comb_func,
                             win_len,
                             slide_len,
                             triggering_delay,
                             winType,
                             pardegree,
                             batch_len,
                             gpu_id,
                             n_thread_block,
                             rebuild, name,
                             routing_func);
    }
#endif

    /** 
     *  \brief Method to create the Key_FFAT_GPU operator
     *  
     *  \return a pointer to the created Key_FFAT_GPU operator (to be explicitly deallocated/destroyed)
     */ 
    keyffat_gpu_t *build_ptr()
    {
        return new keyffat_gpu_t(lift_func,
                                 comb_func,
                                 win_len,
                                 slide_len,
                                 triggering_delay,
                                 winType,
                                 pardegree,
                                 batch_len,
                                 gpu_id,
                                 n_thread_block,
                                 rebuild, name,
                                 routing_func);
    }

    /** 
     *  \brief Method to create the Key_FFAT_GPU operator
     *  
     *  \return a unique_ptr to the created Key_FFAT_GPU operator
     */ 
    std::unique_ptr<keyffat_gpu_t> build_unique()
    {
        return std::make_unique<keyffat_gpu_t>(lift_func,
                                               comb_func,
                                               win_len,
                                               slide_len,
                                               triggering_delay,
                                               winType,
                                               pardegree,
                                               batch_len,
                                               gpu_id,
                                               n_thread_block,
                                               rebuild,
                                               name,
                                               routing_func);
    }
};

/** 
 *  \class PaneFarmGPU_Builder
 *  
 *  \brief Builder of the Pane_Farm_GPU operator
 *  
 *  Builder class to ease the creation of the Pane_Farm_GPU operator.
 */ 
template<typename F_t, typename G_t>
class PaneFarmGPU_Builder
{
private:
    F_t func_F;
    G_t func_G;
    // extract the type of the operator to be generated by this builder (with static checks)
    using tuple_t1 = decltype(get_tuple_t_Win(func_F));
    using result_t1 = decltype(get_result_t_Win(func_F));
    using tuple_t2 = decltype(get_tuple_t_WinGPU(func_F));
    using result_t2 = decltype(get_result_t_WinGPU(func_F));
    // static asserts to check the signature
    static_assert(!(std::is_same<tuple_t1, std::false_type>::value && std::is_same<result_t1, std::false_type>::value &&
                    std::is_same<tuple_t2, std::false_type>::value && std::is_same<result_t2, std::false_type>::value),
        "WindFlow Compilation Error - unknown signature passed to the PaneFarmGPU_Builder (first argument, PLQ logic):\n"
        "  Candidate 1 : void(uint64_t, const Iterable<tuple_t> &, result_t &)\n"
        "  Candidate 2 : void(uint64_t, const tuple_t &, result_t &)\n"
        "  , or if the PLQ stage is on GPU\n"
        "  Candidate : __host__ __device__ void(uint64_t, const tuple_t *, size_t, result_t *, char *, size_t)\n");
    using result_t3 = decltype(get_result_t_Win(func_G));
    using result_t4 = decltype(get_result_t_WinGPU(func_G));
    static_assert(!(std::is_same<std::false_type, result_t3>::value && std::is_same<std::false_type, result_t4>::value),
        "WindFlow Compilation Error - unknown signature passed to the PaneFarmGPU_Builder (second argument, WLQ logic):\n"
        "  Candidate 1 : void(uint64_t, const Iterable<result_t> &, result_t &)\n"
        "  Candidate 2 : void(uint64_t, const result_t &, result_t &)\n"
        "  , or if the WLQ stage is on GPU\n"
        "  Candidate : __host__ __device__ void(uint64_t, const result_t *, size_t, result_t *, char *, size_t)\n");
    static_assert(!((!std::is_same<result_t2, std::false_type>::value && !std::is_same<result_t4, std::false_type>::value) ||
                    (!std::is_same<result_t1, std::false_type>::value && !std::is_same<result_t3, std::false_type>::value)),
        "WindFlow Compilation Error - exactly one of the two stages (either PLQ or WLQ) must be on GPU!\n");
    static_assert(std::is_same<result_t1, result_t4>::value && std::is_same<result_t2, result_t3>::value,
        "WindFlow Compilation Error - type mismatch in the PaneFarmGPU_Builder (output type of the PLQ logic must be equal to the input type of the WLQ logic)\n");
    using tuple_t_plq = typename std::conditional<!std::is_same<tuple_t1, std::false_type>::value, tuple_t1, tuple_t2>::type;
    using result_t_plq = typename std::conditional<!std::is_same<result_t1, std::false_type>::value, result_t1, result_t2>::type;
    using gpu_func_t = decltype(get_GPU_F(func_F, func_G));
    static_assert(!std::is_same<gpu_func_t, std::false_type>::value,
        "WindFlow Compilation Error - only functor objects or anonymous lambdas (not plain functions) can be used to create the Pane_Farm_GPU operator\n");
    using panefarm_gpu_t = Pane_Farm_GPU<tuple_t_plq, result_t_plq, gpu_func_t>;
    uint64_t win_len = 1;
    uint64_t slide_len = 1;
    uint64_t triggering_delay = 0;
    win_type_t winType = win_type_t::CB;
    size_t plq_degree = 1;
    size_t wlq_degree = 1;
    size_t batch_len = 1;
    int gpu_id = 0;
    size_t n_thread_block = DEFAULT_CUDA_NUM_THREAD_BLOCK;
    std::string name = "pf_gpu";
    size_t scratchpad_size = 0;
    opt_level_t opt_level = opt_level_t::LEVEL0;

public:
    /** 
     *  \brief Constructor (only one of the two logics can be __host__ __device__)
     *  
     *  \param _func_F the pane procesing logic (PLQ)
     *  \param _func_G the window processing logic (PLQ)
     *  \note
     *  The __host__ __device__ logic must be passed through a callable object (e.g., lambda, functor)
     */ 
    PaneFarmGPU_Builder(F_t _func_F, G_t _func_G): func_F(_func_F), func_G(_func_G) {}

    /** 
     *  \brief Method to specify the configuration for count-based windows
     *  
     *  \param _win_len window length (in no. of tuples)
     *  \param _slide_len slide length (in no. of tuples)
     *  \return the object itself
     */ 
    PaneFarmGPU_Builder<F_t, G_t> &withCBWindows(uint64_t _win_len, uint64_t _slide_len)
    {
        win_len = _win_len;
        slide_len = _slide_len;
        winType = win_type_t::CB;
        return *this;
    }

    /** 
     *  \brief Method to specify the configuration for time-based windows
     *  
     *  \param _win_len window length (in microseconds)
     *  \param _slide_len slide length (in microseconds)
     *  \param _triggering_delay (in microseconds)
     *  \return the object itself
     */ 
    PaneFarmGPU_Builder<F_t, G_t> &withTBWindows(std::chrono::microseconds _win_len,
                                                 std::chrono::microseconds _slide_len,
                                                 std::chrono::microseconds _triggering_delay=std::chrono::microseconds::zero())
    {
        win_len = _win_len.count();
        slide_len = _slide_len.count();
        triggering_delay = _triggering_delay.count();
        winType = win_type_t::TB;
        return *this;
    }

    /** 
     *  \brief Method to specify the parallelism configuration within the Pane_Farm_GPU operator
     *  
     *  \param _plq_degree number of replicas in the PLQ stage
     *  \param _wlq_degree number of replicas in the WLQ stage
     *  \return the object itself
     */ 
    PaneFarmGPU_Builder<F_t, G_t> &withParallelism(size_t _plq_degree, size_t _wlq_degree)
    {
        plq_degree = _plq_degree;
        wlq_degree = _wlq_degree;
        return *this;
    }

    /** 
     *  \brief Method to specify the batch size
     *  
     *  \param _batch_len number of panes/windows in a batch
     *  \return the object itself
     */ 
    PaneFarmGPU_Builder<F_t, G_t> &withBatch(size_t _batch_len)
    {
        batch_len = _batch_len;
        return *this;
    }

    /** 
     *  \brief Method to specify the GPU configuration used to launch kernels
     *  
     *  \param _gpu_id identifier of the chosen GPU device (default zero)
     *  \param _n_thread_block number of threads per block (default is DEFAULT_CUDA_NUM_THREAD_BLOCK)
     *  \return the object itself
     */ 
    PaneFarmGPU_Builder<F_t, G_t> &withGPUConfiguration(int _gpu_id=0, size_t _n_thread_block=DEFAULT_CUDA_NUM_THREAD_BLOCK)
    {
        gpu_id = _gpu_id;
        n_thread_block = _n_thread_block;
        return *this;
    }

    /** 
     *  \brief Method to specify the name of the Pane_Farm_GPU operator
     *  
     *  \param _name string with the name to be given
     *  \return the object itself
     */ 
    PaneFarmGPU_Builder<F_t, G_t> &withName(std::string _name)
    {
        name = _name;
        return *this;
    }

    /** 
     *  \brief Method to specify the size in bytes of the scratchpad memory per CUDA thread
     *  
     *  \param _scratchpad_size size in bytes of the scratchpad area local of a CUDA thread (pre-allocated on the global memory of the GPU)
     *  \return the object itself
     */ 
    PaneFarmGPU_Builder<F_t, G_t> &withScratchpad(size_t _scratchpad_size)
    {
        scratchpad_size = _scratchpad_size;
        return *this;
    }

    /** 
     *  \brief Method to specify the optimization level to build the Pane_Farm_GPU operator
     *  
     *  \param _opt_level (optimization level)
     *  \return the object itself
     */ 
    PaneFarmGPU_Builder<F_t, G_t> &withOptLevel(opt_level_t _opt_level)
    {
        opt_level = _opt_level;
        return *this;
    }

    /** 
     *  \brief Method to prepare the operator for Nesting with Key_Farm_GPU or Win_Farm_GPU
     *  
     *  \return the object itself
     */ 
    PaneFarmGPU_Builder<F_t, G_t> &prepare4Nesting()
    {
        opt_level = opt_level_t::LEVEL2;
        return *this;
    }

#if __cplusplus >= 201703L
    /** 
     *  \brief Method to create the Pane_Farm_GPU operator (only C++17)
     *  
     *  \return a copy of the created Pane_Farm_GPU operator
     */ 
    panefarm_gpu_t build()
    {
        return panefarm_gpu_t(func_F,
                              func_G,
                              win_len,
                              slide_len,
                              triggering_delay,
                              winType,
                              plq_degree,
                              wlq_degree,
                              batch_len,
                              gpu_id,
                              n_thread_block,
                              name,
                              scratchpad_size,
                              true,
                              opt_level);
    }
#endif

    /** 
     *  \brief Method to create the Pane_Farm_GPU operator
     *  
     *  \return a pointer to the created Pane_Farm_GPU operator (to be explicitly deallocated/destroyed)
     */ 
    panefarm_gpu_t *build_ptr()
    {
        return new panefarm_gpu_t(func_F,
                                  func_G,
                                  win_len,
                                  slide_len,
                                  triggering_delay,
                                  winType,
                                  plq_degree,
                                  wlq_degree,
                                  batch_len,
                                  gpu_id,
                                  n_thread_block,
                                  name,
                                  scratchpad_size,
                                  true,
                                  opt_level);
    }

    /** 
     *  \brief Method to create the Pane_Farm_GPU operator
     *  
     *  \return a unique_ptr to the created Pane_Farm_GPU operator
     */ 
    std::unique_ptr<panefarm_gpu_t> build_unique()
    {
        return std::make_unique<panefarm_gpu_t>(func_F,
                                                func_G,
                                                win_len,
                                                slide_len,
                                                triggering_delay,
                                                winType,
                                                plq_degree,
                                                wlq_degree,
                                                batch_len,
                                                gpu_id,
                                                n_thread_block,
                                                name,
                                                scratchpad_size,
                                                true,
                                                opt_level);
    }
};

/** 
 *  \class WinMapReduceGPU_Builder
 *  
 *  \brief Builder of the Win_MapReduce_GPU operator
 *  
 *  Builder class to ease the creation of the Win_MapReduce_GPU operator.
 */ 
template<typename F_t, typename G_t>
class WinMapReduceGPU_Builder
{
private:
    F_t func_F;
    G_t func_G;
    // extract the type of the operator to be generated by this builder (with static checks)
    using tuple_t1 = decltype(get_tuple_t_Win(func_F));
    using result_t1 = decltype(get_result_t_Win(func_F));
    using tuple_t2 = decltype(get_tuple_t_WinGPU(func_F));
    using result_t2 = decltype(get_result_t_WinGPU(func_F));
    // static asserts to check the signature
    static_assert(!(std::is_same<tuple_t1, std::false_type>::value && std::is_same<result_t1, std::false_type>::value &&
                    std::is_same<tuple_t2, std::false_type>::value && std::is_same<result_t2, std::false_type>::value),
        "WindFlow Compilation Error - unknown signature passed to the WinMapReduceGPU_Builder (first argument, MAP logic):\n"
        "  Candidate 1 : void(uint64_t, const Iterable<tuple_t> &, result_t &)\n"
        "  Candidate 2 : void(uint64_t, const tuple_t &, result_t &)\n"
        "  , or if the MAP stage is on GPU\n"
        "  Candidate : __host__ __device__ void(uint64_t, const tuple_t *, size_t, result_t *, char *, size_t)\n");
    using result_t3 = decltype(get_result_t_Win(func_G));
    using result_t4 = decltype(get_result_t_WinGPU(func_G));
    static_assert(!(std::is_same<std::false_type, result_t3>::value && std::is_same<std::false_type, result_t4>::value),
        "WindFlow Compilation Error - unknown signature passed to the PaneFarmGPU_Builder (second argument, REDUCE logic):\n"
        "  Candidate 1 : void(uint64_t, const Iterable<result_t> &, result_t &)\n"
        "  Candidate 2 : void(uint64_t, const result_t &, result_t &)\n"
        "  , or if the REDUCE stage is on GPU\n"
        "  Candidate : __host__ __device__ void(uint64_t, const result_t *, size_t, result_t *, char *, size_t)\n");
    static_assert(!((!std::is_same<result_t2, std::false_type>::value && !std::is_same<result_t4, std::false_type>::value) ||
                    (!std::is_same<result_t1, std::false_type>::value && !std::is_same<result_t3, std::false_type>::value)),
        "WindFlow Compilation Error - exactly one of the two stages (either PLQ or WLQ) must be on GPU!\n");
    static_assert(std::is_same<result_t1, result_t4>::value && std::is_same<result_t2, result_t3>::value,
        "WindFlow Compilation Error - type mismatch in the PaneFarmGPU_Builder (output type of the MAP logic must be equal to the input type of the REDUCE logic)\n");
    using tuple_t_map = typename std::conditional<!std::is_same<tuple_t1, std::false_type>::value, tuple_t1, tuple_t2>::type;
    using result_t_map = typename std::conditional<!std::is_same<result_t1, std::false_type>::value, result_t1, result_t2>::type;
    using gpu_func_t = decltype(get_GPU_F(func_F, func_G));
    static_assert(!std::is_same<gpu_func_t, std::false_type>::value,
        "WindFlow Compilation Error - only functor objects or anonymous lambdas (not plain functions) can be used to create the Win_MapReduce_GPU operator\n");
    using winmapreduce_gpu_t = Win_MapReduce_GPU<tuple_t_map, result_t_map, gpu_func_t>;
    uint64_t win_len = 1;
    uint64_t slide_len = 1;
    uint64_t triggering_delay = 0;
    win_type_t winType = win_type_t::CB;
    size_t map_degree = 2;
    size_t reduce_degree = 1;
    size_t batch_len = 1;
    int gpu_id = 0;
    size_t n_thread_block = DEFAULT_CUDA_NUM_THREAD_BLOCK;
    std::string name = "wmw_gpu";
    size_t scratchpad_size = 0;
    opt_level_t opt_level = opt_level_t::LEVEL0;

public:
    /** 
     *  \brief Constructor (only one of the two logics can be __host__ __device__)
     *  
     *  \param _func_F the window map logic (MAP)
     *  \param _func_G the window reduce logic (REDUCE)
     *  \note
     *  The __host__ __device__ logic must be passed through a callable object (e.g., lambda, functor)
     */ 
    WinMapReduceGPU_Builder(F_t _func_F, G_t _func_G): func_F(_func_F), func_G(_func_G) {}

    /** 
     *  \brief Method to specify the configuration for count-based windows
     *  
     *  \param _win_len window length (in no. of tuples)
     *  \param _slide_len slide length (in no. of tuples)
     *  \return the object itself
     */ 
    WinMapReduceGPU_Builder<F_t, G_t> &withCBWindows(uint64_t _win_len, uint64_t _slide_len)
    {
        win_len = _win_len;
        slide_len = _slide_len;
        winType = win_type_t::CB;
        return *this;
    }

    /** 
     *  \brief Method to specify the configuration for time-based windows
     *  
     *  \param _win_len window length (in microseconds)
     *  \param _slide_len slide length (in microseconds)
     *  \param _triggering_delay (in microseconds)
     *  \return the object itself
     */ 
    WinMapReduceGPU_Builder<F_t, G_t> &withTBWindows(std::chrono::microseconds _win_len,
                                                     std::chrono::microseconds _slide_len,
                                                     std::chrono::microseconds _triggering_delay=std::chrono::microseconds::zero())
    {
        win_len = _win_len.count();
        slide_len = _slide_len.count();
        triggering_delay = _triggering_delay.count();
        winType = win_type_t::TB;
        return *this;
    }

    /** 
     *  \brief Method to specify the parallelism configuration within the Win_MapReduce_GPU operator
     *  
     *  \param _map_degree number of replicas in the MAP stage
     *  \param _reduce_degree number of replicas in the REDUCE stage
     *  \return the object itself
     */ 
    WinMapReduceGPU_Builder<F_t, G_t> &withParallelism(size_t _map_degree, size_t _reduce_degree)
    {
        map_degree = _map_degree;
        reduce_degree = _reduce_degree;
        return *this;
    }

    /** 
     *  \brief Method to specify the batch size
     *  
     *  \param _batch_len number of partitions/windows in a batch
     *  \return the object itself
     */ 
    WinMapReduceGPU_Builder<F_t, G_t> &withBatch(size_t _batch_len)
    {
        batch_len = _batch_len;
        return *this;
    }

    /** 
     *  \brief Method to specify the GPU configuration used to launch kernels
     *  
     *  \param _gpu_id identifier of the chosen GPU device (default zero)
     *  \param _n_thread_block number of threads per block (default is DEFAULT_CUDA_NUM_THREAD_BLOCK)
     *  \return the object itself
     */ 
    WinMapReduceGPU_Builder<F_t, G_t> &withGPUConfiguration(int _gpu_id=0, size_t _n_thread_block=DEFAULT_CUDA_NUM_THREAD_BLOCK)
    {
        gpu_id = _gpu_id;
        n_thread_block = _n_thread_block;
        return *this;
    }

    /** 
     *  \brief Method to specify the name of the Win_MapReduce_GPU operator
     *  
     *  \param _name string with the name to be given
     *  \return the object itself
     */ 
    WinMapReduceGPU_Builder<F_t, G_t> &withName(std::string _name)
    {
        name = _name;
        return *this;
    }

    /** 
     *  \brief Method to specify the size in bytes of the scratchpad memory per CUDA thread
     *  
     *  \param _scratchpad_size size in bytes of the scratchpad area local of a CUDA thread (pre-allocated on the global memory of the GPU)
     *  \return the object itself
     */ 
    WinMapReduceGPU_Builder<F_t, G_t> &withScratchpad(size_t _scratchpad_size)
    {
        scratchpad_size = _scratchpad_size;
        return *this;
    }

    /** 
     *  \brief Method to specify the optimization level to build the Win_MapReduce_GPU operator
     *  
     *  \param _opt_level (optimization level)
     *  \return the object itself
     */ 
    WinMapReduceGPU_Builder<F_t, G_t> &withOptLevel(opt_level_t _opt_level)
    {
        opt_level = _opt_level;
        return *this;
    }

    /** 
     *  \brief Method to prepare the operator for Nesting with Key_Farm_GPU or Win_Farm_GPU
     *  
     *  \return the object itself
     */ 
    WinMapReduceGPU_Builder<F_t, G_t> &prepare4Nesting()
    {
        opt_level = opt_level_t::LEVEL2;
        return *this;
    }

#if __cplusplus >= 201703L
    /** 
     *  \brief Method to create the Win_MapReduce_GPU operator (only C++17)
     *  
     *  \return a copy of the created Win_MapReduce_GPU operator
     */ 
    winmapreduce_gpu_t build()
    {
        return winmapreduce_gpu_t(func_F,
                                  func_G,
                                  win_len,
                                  slide_len,
                                  triggering_delay,
                                  winType,
                                  map_degree,
                                  reduce_degree,
                                  batch_len,
                                  gpu_id,
                                  n_thread_block,
                                  name,
                                  scratchpad_size,
                                  true,
                                  opt_level);
    }
#endif

    /** 
     *  \brief Method to create the Win_MapReduce_GPU operator
     *  
     *  \return a pointer to the created Win_MapReduce_GPU operator (to be explicitly deallocated/destroyed)
     */ 
    winmapreduce_gpu_t *build_ptr()
    {
        return new winmapreduce_gpu_t(func_F,
                                      func_G,
                                      win_len,
                                      slide_len,
                                      triggering_delay,
                                      winType,
                                      map_degree,
                                      reduce_degree,
                                      batch_len,
                                      gpu_id,
                                      n_thread_block,
                                      name,
                                      scratchpad_size,
                                      true,
                                      opt_level);
    }

    /** 
     *  \brief Method to create the Win_MapReduce_GPU operator
     *  
     *  \return a unique_ptr to the created Win_MapReduce_GPU operator
     */ 
    std::unique_ptr<winmapreduce_gpu_t> build_unique()
    {
        return std::make_unique<winmapreduce_gpu_t>(func_F,
                                                    func_G,
                                                    win_len,
                                                    slide_len,
                                                    triggering_delay,
                                                    winType,
                                                    map_degree,
                                                    reduce_degree,
                                                    batch_len,
                                                    gpu_id,
                                                    n_thread_block,
                                                    name,
                                                    scratchpad_size,
                                                    true,
                                                    opt_level);
    }
};

} // namespace wf

#endif
